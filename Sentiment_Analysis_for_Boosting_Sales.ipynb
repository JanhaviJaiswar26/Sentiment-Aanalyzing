{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yJVHSHyRRjzX"
      },
      "outputs": [],
      "source": [
        "# Import Libraries\n",
        "!pip install vaderSentiment\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Download NLTK data\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Load the dataset\n",
        "# Replace 'amazon_reviews.csv' with your dataset path\n",
        "df = pd.read_csv('/content/ama.csv')\n",
        "\n",
        "# Display first few rows\n",
        "print(\"Initial Data:\")\n",
        "print(df.head())\n",
        "\n",
        "#------------------------------------------------------------------\n",
        "\n",
        "\n",
        "#if 'reviewText' not in df.columns:\n",
        "   # if 'Review Text' in df.columns:\n",
        "    #    df = df.rename(columns={'Review Text': 'review_content'})  # Rename column to 'reviewText'\n",
        "    #else:\n",
        "     #   raise KeyError(\"Neither 'reviewText' nor 'Review Text' column found in the DataFrame.\")\n",
        "\n",
        "\n",
        "# Data Cleaning Function\n",
        "def clean_text(text):\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
        "    # Remove mentions and hashtags\n",
        "    text = re.sub(r'\\@\\w+|\\#', '', text)\n",
        "    # Remove non-alphabetic characters\n",
        "    text = re.sub(r'[^A-Za-z\\s]', '', text)\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    return text\n",
        "\n",
        "# Apply cleaning\n",
        "df['cleaned_review'] = df['review_content'].apply(lambda x: clean_text(str(x)))\n",
        "\n",
        "# Initialize stopwords and lemmatizer\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Preprocessing Function\n",
        "def preprocess_text(text):\n",
        "    # Tokenize\n",
        "    tokens = text.split()\n",
        "    # Remove stopwords\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "    # Lemmatize\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "# Apply preprocessing\n",
        "df['processed_review'] = df['cleaned_review'].apply(preprocess_text)\n",
        "\n",
        "# Handle missing values\n",
        "print(\"\\nMissing Values Before:\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "df = df.dropna(subset=['review_content', 'rating_count'])\n",
        "\n",
        "print(\"\\nMissing Values After:\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# Initialize VADER\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Function to get sentiment score\n",
        "def get_sentiment_score(text):\n",
        "    score = analyzer.polarity_scores(text)\n",
        "    return score['compound']\n",
        "\n",
        "# Apply sentiment analysis\n",
        "df['sentiment_score'] = df['processed_review'].apply(get_sentiment_score)\n",
        "\n",
        "# Categorize sentiments\n",
        "def categorize_sentiment(score):\n",
        "    if score >= 0.05:\n",
        "        return 'positive'\n",
        "    elif score <= -0.05:\n",
        "        return 'negative'\n",
        "    else:\n",
        "        return 'neutral'\n",
        "\n",
        "df['sentiment'] = df['sentiment_score'].apply(categorize_sentiment)\n",
        "\n",
        "# Display sentiment distribution\n",
        "print(\"\\nSentiment Distribution:\")\n",
        "print(df['sentiment'].value_counts())\n",
        "\n",
        "# Convert reviewTime to datetime\n",
        "#df['reviewTime'] = pd.to_datetime(df['reviewTime'], format='%m %d, %Y')\n",
        "df['month'] = df['reviewTime'].dt.to_period('M')\n",
        "\n",
        "# Aggregate data by month\n",
        "monthly_reviews = df.groupby('month').agg({\n",
        "    'sentiment_score': 'mean',\n",
        "    'sentiment': 'count'\n",
        "}).rename(columns={'sentiment': 'review_count'})\n",
        "\n",
        "# Generate synthetic sales data\n",
        "np.random.seed(42)\n",
        "monthly_reviews['sales'] = monthly_reviews['review_count'] * 10 + (monthly_reviews['sentiment_score'] * 100) + np.random.randint(-100, 100, size=monthly_reviews.shape[0])\n",
        "\n",
        "# Display monthly aggregated data\n",
        "print(\"\\nMonthly Aggregated Data:\")\n",
        "print(monthly_reviews.head())\n",
        "\n",
        "# Calculate correlation\n",
        "correlation = monthly_reviews['sentiment_score'].corr(monthly_reviews['sales'])\n",
        "print(f\"\\nCorrelation between sentiment score and sales: {correlation:.2f}\")\n",
        "\n",
        "# Visualization\n",
        "\n",
        "# a. Sentiment Distribution\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.countplot(x='sentiment', data=df, palette='viridis')\n",
        "plt.title('Sentiment Distribution')\n",
        "plt.xlabel('Sentiment')\n",
        "plt.ylabel('Count')\n",
        "plt.show()\n",
        "\n",
        "# b. Average Monthly Sentiment Score\n",
        "plt.figure(figsize=(12,6))\n",
        "monthly_sentiment = monthly_reviews['sentiment_score']\n",
        "monthly_sentiment.plot(kind='line', marker='o')\n",
        "plt.title('Average Monthly Sentiment Score')\n",
        "plt.xlabel('Month')\n",
        "plt.ylabel('Average Sentiment Score')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# c. Monthly Sales\n",
        "plt.figure(figsize=(12,6))\n",
        "monthly_sales = monthly_reviews['sales']\n",
        "monthly_sales.plot(kind='line', marker='o', color='orange')\n",
        "plt.title('Monthly Sales')\n",
        "plt.xlabel('Month')\n",
        "plt.ylabel('Sales')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# d. Sentiment Score vs. Sales\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.scatterplot(x='sentiment_score', y='sales', data=monthly_reviews, hue=monthly_reviews.index.astype(str))\n",
        "plt.title('Sentiment Score vs. Sales')\n",
        "plt.xlabel('Average Sentiment Score')\n",
        "plt.ylabel('Sales')\n",
        "plt.legend(title='Month', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# e. Correlation Heatmap\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.heatmap(monthly_reviews[['sentiment_score', 'sales']].corr(), annot=True, cmap='coolwarm')\n",
        "plt.title('Correlation Heatmap')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/ama.csv')\n",
        "df"
      ],
      "metadata": {
        "id": "L6Baht4ST3HX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "file_path = '/content/ama.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Display the first few rows and basic info about the data\n",
        "print(\"Data Sample:\")\n",
        "print(data.head())\n",
        "print(\"\\nData Info:\")\n",
        "print(data.info())\n"
      ],
      "metadata": {
        "id": "ROq6gIvbT5st"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop columns that aren't useful for sentiment analysis (modify based on actual data)\n",
        "data = data.drop(columns=['column_to_drop'], errors='ignore')\n",
        "\n",
        "# Handle missing values (remove rows with NaN in important columns like review text)\n",
        "data = data.dropna(subset=['review_content'])  # Replace 'review_body' with the actual review text column name\n"
      ],
      "metadata": {
        "id": "EmqjWWAzV6q3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "import nltk\n",
        "\n",
        "# Download the 'punkt' resource\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Preprocess text data\n",
        "def preprocess_text(text):\n",
        "    # Lowercase the text\n",
        "    text = text.lower()\n",
        "    # Remove special characters and numbers\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "    # Tokenize words\n",
        "    words = word_tokenize(text)\n",
        "    # Remove stopwords and apply stemming\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    stemmer = PorterStemmer()\n",
        "    processed_text = [stemmer.stem(word) for word in words if word not in stop_words]\n",
        "    return ' '.join(processed_text)\n",
        "\n",
        "# Apply preprocessing to the review column\n",
        "data['processed_review'] = data['review_content'].apply(preprocess_text)  # Replace 'review_body' with the actual column name\n"
      ],
      "metadata": {
        "id": "v6FyDB66V772"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "import nltk\n",
        "\n",
        "# Download VADER lexicon for sentiment analysis\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "# Initialize VADER sentiment analyzer\n",
        "sid = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Calculate sentiment scores for each review\n",
        "data['sentiment_score'] = data['processed_review'].apply(lambda x: sid.polarity_scores(x)['compound'])\n",
        "\n",
        "# Classify sentiment as Positive, Neutral, or Negative\n",
        "data['sentiment'] = data['sentiment_score'].apply(lambda score: 'Positive' if score > 0.05 else ('Negative' if score < -0.05 else 'Neutral'))\n"
      ],
      "metadata": {
        "id": "tVhGf7UKWjsb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['rating'] = pd.to_numeric(data['rating'], errors='coerce')\n",
        "\n",
        "# Drop rows with NaN values in 'rating' column\n",
        "data = data.dropna(subset=['rating'])\n",
        "\n",
        "correlation = data[['sentiment_score', 'rating']].corr()  # Replace 'rating' with the actual column name\n",
        "print(\"\\nCorrelation between Sentiment Score and Rating:\")\n",
        "print(correlation)"
      ],
      "metadata": {
        "id": "2vzEkFu0Wq0A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Sentiment distribution\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.countplot(data=data, x='sentiment')\n",
        "plt.title('Sentiment Distribution')\n",
        "plt.xlabel('Sentiment')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()\n",
        "\n",
        "# Scatter plot for sentiment score vs rating\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(data=data, x='sentiment_score', y='rating', hue='sentiment')\n",
        "plt.title('Sentiment Score vs Rating')\n",
        "plt.xlabel('Sentiment Score')\n",
        "plt.ylabel('Rating')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "BG3GYZV2WrT-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#final\n",
        "\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Load Dataset\n",
        "data = pd.read_csv('/content/ama.csv')\n",
        "\n",
        "\n",
        "# Display sample data and structure\n",
        "print(\"Data Sample:\")\n",
        "print(data.head())\n",
        "print(\"\\nData Info:\")\n",
        "print(data.info())\n",
        "\n",
        "# Data Cleaning - Drop rows with missing values in essential columns\n",
        "print(data.isna().sum())\n",
        "data = data.dropna(subset=['rating_count'])  # Replace with actual review and rating column names\n",
        "\n",
        "# Step 3: Text Preprocessing\n",
        "nltk.download('stopwords')\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Lowercase the text\n",
        "    text = text.lower()\n",
        "    # Remove special characters and numbers\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "    # Tokenize words\n",
        "    words = word_tokenize(text)\n",
        "    # Remove stopwords and apply stemming\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    stemmer = PorterStemmer()\n",
        "    processed_text = [stemmer.stem(word) for word in words if word not in stop_words]\n",
        "    return ' '.join(processed_text)\n",
        "\n",
        "# Apply preprocessing to the review text column\n",
        "data['processed_review'] = data['review_content'].apply(preprocess_text)  # Replace 'review_body' with actual column name\n",
        "\n",
        "# Step 4: Sentiment Analysis using VADER\n",
        "sid = SentimentIntensityAnalyzer()\n",
        "data['sentiment_score'] = data['processed_review'].apply(lambda x: sid.polarity_scores(x)['compound'])\n",
        "\n",
        "# Classify sentiment based on the compound score\n",
        "data['sentiment'] = data['sentiment_score'].apply(lambda score: 'Positive' if score > 0.05 else ('Negative' if score < -0.05 else 'Neutral'))\n",
        "\n",
        "# Step 5: Sales Insights\n",
        "# Correlation between sentiment score and rating\n",
        "data['rating'] = pd.to_numeric(data['rating'], errors='coerce')\n",
        "data = data.dropna(subset=['rating'])\n",
        "\n",
        "\n",
        "\n",
        "correlation = data[['sentiment_score', 'rating']].corr()  # Replace 'rating' with actual rating column name\n",
        "print(\"\\nCorrelation between Sentiment Score and Rating:\")\n",
        "print(correlation)\n",
        "\n",
        "# Average rating and sentiment score per sentiment category\n",
        "sentiment_analysis = data.groupby('sentiment').agg(\n",
        "    avg_rating=('rating', 'mean'),\n",
        "    avg_sentiment_score=('sentiment_score', 'mean'),\n",
        "    count=('sentiment', 'count')\n",
        ").reset_index()\n",
        "print(\"\\nSentiment Analysis Summary:\")\n",
        "print(sentiment_analysis)\n",
        "\n",
        "# Step 6: Visualization\n",
        "\n",
        "# Sentiment distribution\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.countplot(data=data, x='sentiment', palette='viridis')\n",
        "plt.title('Sentiment Distribution')\n",
        "plt.xlabel('Sentiment')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()\n",
        "\n",
        "# Scatter plot for sentiment score vs rating\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(data=data, x='sentiment_score', y='rating', hue='sentiment', palette='coolwarm')\n",
        "plt.title('Sentiment Score vs Rating')\n",
        "plt.xlabel('Sentiment Score')\n",
        "plt.ylabel('Rating')\n",
        "plt.show()\n",
        "\n",
        "# Average rating by sentiment category\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(data=sentiment_analysis, x='sentiment', y='avg_rating', palette='viridis')\n",
        "plt.title('Average Rating by Sentiment Category')\n",
        "plt.xlabel('Sentiment')\n",
        "plt.ylabel('Average Rating')\n",
        "plt.show()\n",
        "\n",
        "# Step 7: Insights and Recommendations\n",
        "\n",
        "# Positive Insights: Highlight features or attributes mentioned in positive reviews\n",
        "positive_reviews = data[data['sentiment'] == 'Positive']['review_content']\n",
        "# Display sample positive reviews (for more detail, perform keyword analysis on these reviews)\n",
        "print(\"\\nSample Positive Reviews:\")\n",
        "print(positive_reviews.sample(5, random_state=1))\n",
        "\n",
        "# Negative Insights: Identify common complaints in negative reviews\n",
        "negative_reviews = data[data['sentiment'] == 'Negative']['review_content']\n",
        "print(\"\\nSample Negative Reviews:\")\n",
        "print(negative_reviews.sample(5, random_state=1))\n",
        "\n",
        "# Save analyzed data to CSV for further exploration if needed\n",
        "output_path = '/content/analyzed_amazon_reviews.csv'\n",
        "data.to_csv(output_path, index=False)\n",
        "print(f\"\\nCleaned and analyzed data saved to {output_path}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "R92417bUnGiX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract key words/phrases from positive reviews\n",
        "positive_reviews = data[data['sentiment'] == 'Positive']\n",
        "top_features = positive_reviews['processed_review'].str.split().explode().value_counts().head(10)\n",
        "\n",
        "# Display top 10 words/phrases from positive sentiment reviews\n",
        "print(\"Top Features in Positive Reviews:\")\n",
        "print(top_features)\n"
      ],
      "metadata": {
        "id": "jX6Bys7PqSE0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract common complaints/feedback from negative reviews\n",
        "negative_reviews = data[data['sentiment'] == 'Negative']\n",
        "complaints = negative_reviews['processed_review'].str.split().explode().value_counts().head(10)\n",
        "\n",
        "# Display top 10 complaints/feedback from negative sentiment reviews\n",
        "print(\"Top Complaints in Negative Reviews:\")\n",
        "print(complaints)\n"
      ],
      "metadata": {
        "id": "j18mE5AUqSyP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Segment positive and negative sentiment customers\n",
        "positive_customers = data[data['sentiment'] == 'Positive']\n",
        "negative_customers = data[data['sentiment'] == 'Negative']\n",
        "\n",
        "# Create marketing campaigns or follow-up actions for these segments\n",
        "print(\"\\nSample Positive Customers:\")\n",
        "print(positive_customers[['product_id', 'review_content', 'sentiment_score']].head(5))\n",
        "\n",
        "print(\"\\nSample Negative Customers:\")\n",
        "print(negative_customers[['product_id', 'review_content', 'sentiment_score']].head(5))\n"
      ],
      "metadata": {
        "id": "_iDq1hO_sjCj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from collections import Counter\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Define a list of keywords to analyze\n",
        "keywords = ['price', 'quality', 'service']\n",
        "\n",
        "# Function to count occurrences of keywords in reviews\n",
        "def count_keywords_in_review(text, keywords):\n",
        "    words = word_tokenize(text.lower())  # Tokenize and convert to lowercase\n",
        "    keyword_counts = {keyword: words.count(keyword) for keyword in keywords}\n",
        "    return keyword_counts\n",
        "\n",
        "# Clean reviews by dropping missing data and ensuring valid content\n",
        "positive_reviews = positive_reviews.dropna(subset=['processed_review'])\n",
        "negative_reviews = negative_reviews.dropna(subset=['processed_review'])\n",
        "\n",
        "# Apply the keyword counting function to positive and negative reviews\n",
        "positive_reviews['keyword_counts'] = positive_reviews['processed_review'].apply(count_keywords_in_review, keywords=keywords)\n",
        "negative_reviews['keyword_counts'] = negative_reviews['processed_review'].apply(count_keywords_in_review, keywords=keywords)\n",
        "\n",
        "# Summarize the total occurrences of each keyword in positive and negative reviews\n",
        "positive_keyword_counts = pd.DataFrame(positive_reviews['keyword_counts'].tolist(), columns=keywords)\n",
        "negative_keyword_counts = pd.DataFrame(negative_reviews['keyword_counts'].tolist(), columns=keywords)\n",
        "\n",
        "# Total counts for positive and negative reviews\n",
        "positive_totals = positive_keyword_counts.sum()\n",
        "negative_totals = negative_keyword_counts.sum()\n",
        "\n",
        "# Combine positive and negative keyword counts for visualization\n",
        "keyword_data = pd.DataFrame({\n",
        "    'Positive': positive_totals,\n",
        "    'Negative': negative_totals\n",
        "})\n",
        "\n",
        "# Optionally: Add Neutral sentiment category and count for more insight (if neutral reviews are available)\n",
        "# Assuming `sentiment` is a column in your reviews DataFrame\n",
        "neutral_reviews = data[data['sentiment'] == 'Neutral']\n",
        "neutral_reviews['keyword_counts'] = neutral_reviews['processed_review'].apply(count_keywords_in_review, keywords=keywords)\n",
        "neutral_keyword_counts = pd.DataFrame(neutral_reviews['keyword_counts'].tolist(), columns=keywords)\n",
        "neutral_totals = neutral_keyword_counts.sum()\n",
        "\n",
        "# Add Neutral data to the keyword_data\n",
        "keyword_data['Neutral'] = neutral_totals\n",
        "\n",
        "# Plot the keyword frequencies in positive, negative, and neutral reviews\n",
        "plt.figure(figsize=(10, 6))\n",
        "keyword_data.plot(kind='bar', color=['green', 'red', 'gray'], width=0.8)\n",
        "plt.title('Keyword Frequency Analysis in Sentiment Categories')\n",
        "plt.xlabel('Keyword')\n",
        "plt.ylabel('Frequency')\n",
        "plt.xticks(rotation=0)\n",
        "plt.legend(title='Sentiment', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "plt.tight_layout()  # Adjust layout to prevent clipping of labels\n",
        "plt.show()\n",
        "\n",
        "# Display keyword frequency counts for each sentiment\n",
        "print(\"\\nTotal Keyword Counts in Positive Reviews:\")\n",
        "print(positive_totals)\n",
        "\n",
        "print(\"\\nTotal Keyword Counts in Negative Reviews:\")\n",
        "print(negative_totals)\n",
        "\n",
        "print(\"\\nTotal Keyword Counts in Neutral Reviews:\")\n",
        "print(neutral_totals)\n",
        "\n",
        "# Optionally: Find sentences with high frequency keywords for context-based analysis\n",
        "def find_keyword_sentences(text, keywords):\n",
        "    sentences = nltk.sent_tokenize(text)\n",
        "    keyword_sentences = [sentence for sentence in sentences if any(keyword in sentence.lower() for keyword in keywords)]\n",
        "    return keyword_sentences\n",
        "\n",
        "# Example: Find and display some keyword-related sentences from positive and negative reviews\n",
        "positive_reviews['keyword_sentences'] = positive_reviews['review_content'].apply(lambda x: find_keyword_sentences(x, keywords))\n",
        "negative_reviews['keyword_sentences'] = negative_reviews['review_content'].apply(lambda x: find_keyword_sentences(x, keywords))\n",
        "\n",
        "# Display some sample sentences containing the keywords\n",
        "print(\"\\nSample Sentences with Keywords in Positive Reviews:\")\n",
        "print(positive_reviews['keyword_sentences'].head())\n",
        "\n",
        "print(\"\\nSample Sentences with Keywords in Negative Reviews:\")\n",
        "print(negative_reviews['keyword_sentences'].head())\n"
      ],
      "metadata": {
        "id": "R1VRseg8uj9Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "niDizyoKvKmA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}